from github import Github
from datetime import datetime
from dotenv import load_dotenv
import os
import re
from ollama import chat, ChatResponse

# Load environment variables from .env file
load_dotenv()

# Access the GitHub token
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
REPO_NAME = "MaximeGloesener/demo"  # ex: "octocat/Hello-World"
START_DATE = "2024-09-20"
END_DATE = "2024-09-24"
TARGET_AUTHOR = "MaximeGloesener"  # pour filtrer uniquement les commits de l'utilisateur

# Connexion √† l'API
g = Github(GITHUB_TOKEN)
repo = g.get_repo(REPO_NAME)

# Conversion des dates
start = datetime.fromisoformat(START_DATE)
end = datetime.fromisoformat(END_DATE)

# R√©cup√©rer les commits
commits = repo.get_commits(since=start, until=end)

CODE_EXTENSIONS = {
    ".py", ".js", ".ts", ".java", ".cpp", ".c", ".cs", ".go",
    ".rs", ".rb", ".php", ".html", ".css", ".scss", ".vue",
    ".swift", ".kt", ".m", ".sh", ".sql"
}

def is_code_file(filename: str) -> bool:
    _, ext = os.path.splitext(filename)
    return ext.lower() in CODE_EXTENSIONS

def calculate_commit_stats(commit_data: list[dict]) -> dict:
    """Calculate statistics from commit data."""
    total_additions = 0
    total_deletions = 0
    total_files = 0
    file_types = set()

    for commit in commit_data:
        for file in commit["files"]:
            total_additions += file["additions"]
            total_deletions += file["deletions"]
            total_files += 1
            _, ext = os.path.splitext(file["filename"])
            file_types.add(ext.lower())

    return {
        "total_commits": len(commit_data),
        "total_additions": total_additions,
        "total_deletions": total_deletions,
        "total_files": total_files,
        "file_types": list(file_types)
    }
def format_prompt(challenge_description: str, commits: list[dict], start_date: str, end_date: str, stats: dict) -> str:
    prompt = f"""Tu es un assistant expert en revue de code. Ton r√¥le est d'√©valuer si un d√©veloppeur a respect√© un objectif annonc√© dans un challenge de codage.

*** OBJECTIF DU CHALLENGE ***
üéØ L'objectif du challenge est le suivant:
{challenge_description}

*** CODE IMPLEMENTE ***
üìÇ Voici les commits et les changements associ√©s (fichiers de code uniquement) :
"""

    for commit in commits:
        prompt += f"\n---\nüîê Commit {commit['sha'][:7]} - {commit['date']}\nüìù Message: {commit['message']}\n"
        for file in commit["files"]:
            prompt += f"\nüìÑ Fichier: {file['filename']} (+{file['additions']} / -{file['deletions']})\n"
            if file["patch"]:
                prompt += f"diff\n{file['patch']}\n\n"

    prompt += """
---

Tu connais maintenant l'objectif du challenge, c'est-√†-dire ce qui est attendu du d√©veloppeur. Et tu connais √©galement le code impl√©ment√©, pour savoir si le challenge est r√©ussi ou non, tu vas suivre les √©tapes suivantes:

üß† √âtape 1 : Donne un **r√©sum√© technique clair** de ce qui a √©t√© fait dans le code (ce que le d√©veloppeur a r√©ellement produit).

üß™ √âtape 2 : V√©rifie **si le travail accompli correspond exactement √† l'objectif du challenge**.
- Liste les mots-cl√©s et concepts sp√©cifiques de l'objectif du challenge
- Liste les mots-cl√©s et concepts sp√©cifiques du r√©sum√© technique
- V√©rifie s'il y a une correspondance directe entre ces deux ensembles de concepts
- Identifie clairement tout √©cart ou divergence entre le r√©sum√© technique et l'objectif

ATTENTION CRITIQUE: Tu dois faire une comparaison terme √† terme entre l'objectif du challenge et le r√©sum√© technique. Pour qu'un challenge soit consid√©r√© comme respect√©, le domaine technique et la fonctionnalit√© principale DOIVENT correspondre exactement.

Exemples d'incompatibilit√©:
- Si l'objectif mentionne "page web m√©t√©o" et le code concerne "mod√®le de pruning", alors le challenge n'est PAS respect√©.
- Si l'objectif mentionne "API REST" et le code impl√©mente "interface graphique", alors le challenge n'est PAS respect√©.
- Si l'objectif mentionne "base de donn√©es SQL" et le code impl√©mente "stockage NoSQL", alors le challenge n'est PAS respect√©.

üìä √âtape 3 : Donne une **note sur 10** uniquement si le code correspond √† l'objectif. Sinon, donne une note de 0 et explique clairement pourquoi.

Format obligatoire de ta r√©ponse :

- R√©sum√© technique :
- √âvaluation :
  - Mots-cl√©s de l'objectif: [liste]
  - Mots-cl√©s du r√©sum√©: [liste]
  - Correspondance: [OUI/NON avec explication]
- NOTE: X
"""

    return prompt


def extract_score(llm_response: str) -> int:
    match = re.search(r"NOTE:\s*([0-9]|10)\b", llm_response)
    if match:
        score = int(match.group(1))
        # Validation que le score est dans la plage valide
        if 0 <= score <= 10:
            return score
    return None

def extract_summary(llm_response: str) -> dict:
    parts = re.split(r"(R√©sum√© technique\s*:|√âvaluation\s*:|NOTE\s*:)", llm_response, flags=re.IGNORECASE)
    return {
        "summary": parts[2].strip() if len(parts) >= 6 else "",
        "evaluation": parts[4].strip() if len(parts) >= 6 else "",
        "score": extract_score(llm_response)
    }

def ask_ollama(prompt: str) -> str:
    response: ChatResponse = chat(model='qwen3:4b', messages=[
        {"role": "system", "content": "Tu es un assistant expert en revue de code."},
        {"role": "user", "content": prompt}
    ])
    return response['message']['content']

def evaluate_challenge(
    repo_name: str,
    start_date: str,
    end_date: str,
    target_author: str,
    challenge_description: str,
    github_token: str = None
) -> dict:
    """
    Evaluate a coding challenge based on GitHub commits.

    Args:
        repo_name: GitHub repository name (format: "owner/repo")
        start_date: Start date in ISO format (YYYY-MM-DD)
        end_date: End date in ISO format (YYYY-MM-DD)
        target_author: GitHub username to filter commits
        challenge_description: Description of the coding challenge
        github_token: GitHub API token (optional if set in .env)

    Returns:
        dict: Contains summary, evaluation, score, and statistics
    """
    token = github_token or os.getenv("GITHUB_TOKEN")
    if not token:
        raise ValueError("GitHub token is required")

    g = Github(token)
    repo = g.get_repo(repo_name)
    start = datetime.fromisoformat(start_date)
    end = datetime.fromisoformat(end_date)
    commits = repo.get_commits(since=start, until=end)

    commit_data = []
    for commit in commits:
        if commit.author and commit.author.login == target_author:
            files = [
                {
                    "filename": file.filename,
                    "additions": file.additions,
                    "deletions": file.deletions,
                    "patch": file.patch
                }
                for file in commit.files
                if is_code_file(file.filename)
            ]

            if files:
                commit_data.append({
                    "sha": commit.sha,
                    "date": commit.commit.author.date.isoformat(),
                    "message": commit.commit.message,
                    "files": files
                })

    # Si aucun commit n'est trouv√©, retourner un r√©sultat d'√©chec
    if not commit_data:
        return {
            "summary": "Aucun commit trouv√© dans la p√©riode sp√©cifi√©e.",
            "evaluation": "Le challenge n'a pas √©t√© compl√©t√© car aucun commit n'a √©t√© effectu√© pendant la p√©riode donn√©e.",
            "score": 0,
            "stats": {
                "total_commits": 0,
                "total_additions": 0,
                "total_deletions": 0,
                "total_files": 0,
                "file_types": []
            }
        }

    # Calculer les statistiques
    stats = calculate_commit_stats(commit_data)

    prompt = format_prompt(challenge_description, commit_data, start_date, end_date, stats)
    response = ask_ollama(prompt)
    result = extract_summary(response)

    # Validation suppl√©mentaire pour s'assurer que le score est coh√©rent
    if result['score'] is None or not (0 <= result['score'] <= 10):
        result['score'] = 0
        result['evaluation'] = "Erreur dans l'√©valuation. Score invalide d√©tect√©."

    # Ajouter les statistiques au r√©sultat
    result['stats'] = stats

    return result

# Example usage:
if __name__ == "__main__":
    result = evaluate_challenge(
        repo_name="MaximeGloesener/demo",
        start_date="2024-09-10",
        end_date="2024-09-24",
        target_author="MaximeGloesener",
        challenge_description="faire une application qui fait la m√©t√©o en react JS"
    )

    print("R√©sum√© technique :", result['summary'])
    print("√âvaluation :", result['evaluation'])
    print("Note :", result['score'], "/ 10")
    print("\nStatistiques :")
    print(f"- Nombre de commits : {result['stats']['total_commits']}")
    print(f"- Lignes ajout√©es : {result['stats']['total_additions']}")
    print(f"- Lignes supprim√©es : {result['stats']['total_deletions']}")
    print(f"- Fichiers modifi√©s : {result['stats']['total_files']}")
    print(f"- Types de fichiers : {', '.join(result['stats']['file_types'])}")